## Linear Discriminant Analysis

### Descrição & Funcionamento

Apesar das suas vantagens, a regressão logística apresenta maus resultado para classes mal separadas, pos este torna-se bastante instável. O LDA tende a ser mais estável que a regressão logística e é normalmente utilizado para problemas de classificação com mais de duas classes de resposta. 

No algoritmo de LDA, é assumido que os atributos são retirados de uma distribuição normal, com médias específicas por classe e um matriz de covariância comum entre todos os atributos.

### Casos de Estudo

Predetendemos efetuar *backwards feature selection* de forma a aferir os atributos que mais influenciam o modelo produzido, minimizando a média de erros e variância. Assim sendo, para ambos os conjuntos começamos com um modelo que utiliza todos os atributos e incrementalmente reduzimos o número de elementos considerados, desde que estes forneçam melhorias ao nosso modelo. No entanto, podem existir casos em que a diferença de média de erros original e da versão reduzida é residual, nesse caso optamos pelo modelo mais simples. Para estes casos de estudo foi utilizado *cross validation* com $k = 10$ *folds*.

A metodologia de *backwards feature selection* consiste em realizar iterações sendo que uma iteração corresponde aos passos necessários para descobrir o primeiro elemento que leva ao modelo com menor erro. Na iteração seguinte, partimos desse modelo e voltamos a remover todos os atributos até encontrarmos o novo candidato. O caso de paragem corresponde à iteração em que nenhum atributo candidato seja identificado, na qual chegamos ao modelo simplista com menor erro.

#### Vinho Tinto

Considerando um modelo inicial que contempla todos os atributos, somo capazes de obter a seguinte média de erro. Logo, qualquer modelo mais simples que produz uma média menor ou muito semelhante, desde que mais simples, será preferido.

```{r}
r_lda_total = cross_validation_w_lda(red_wine, 10, quality ~ .)
mean(r_lda_total)
```
Abaixo apresentado surge a tabela relativa aos resultados obtidos em cada iteração, indicando a respectiva iteração, atributo removido e média do modelo resultante.


```{r echo=FALSE}
r_lda_1it = cross_validation_w_lda(red_wine, 10, quality ~ .  - fixed.acidity )
r_lda_2it = cross_validation_w_lda(red_wine, 10, quality ~ . - fixed.acidity - residual.sugar )
r_lda_3it = cross_validation_w_lda(red_wine, 10, quality ~ . - fixed.acidity - residual.sugar - pH )
r_lda_4it = cross_validation_w_lda(red_wine, 10, quality ~ . - fixed.acidity - residual.sugar - pH - density )
r_lda_5it = cross_validation_w_lda(red_wine, 10, quality ~ . - fixed.acidity - residual.sugar - pH - density - citric.acid )
r_lda_6it = cross_validation_w_lda(red_wine, 10, quality ~ . - fixed.acidity - residual.sugar - pH - density - citric.acid - free.sulfur.dioxide )
r_lda_7it = cross_validation_w_lda(red_wine, 10, quality ~ . - fixed.acidity - residual.sugar - pH - density - citric.acid - free.sulfur.dioxide - chlorides )

base_it <- c("-", mean(r_lda_total))
lda1it <- c("fixed acidity", mean(r_lda_1it))
lda2it <- c("residual sugar", mean(r_lda_2it))
lda3it <- c("pH", mean(r_lda_3it))
lda4it <- c("density", mean(r_lda_4it))
lda5it <- c("citric acid", mean(r_lda_5it))
lda6it <- c("free sulfur dioxide", mean(r_lda_6it))
lda7it <- c("chlorides", mean(r_lda_7it))
col_names_all = c("Ponto de Partido", "1ª Iteração","2ª Iteração","3ª Iteração","4ª Iteração","5ª Iteração","6ª Iteração", "Última Iteração")
main_df <- data.frame(base_it, lda1it,lda2it,lda3it,lda4it,lda5it,lda6it,lda7it )
names(main_df) <- col_names_all
knitr::kable(t(main_df), col.names=c("Atributo Removido","Precisão Obtida"),caption="Resumo da aplicação de backwards feature selection")
```

Como se pode observar pela tabela acima, a aplicação do método de *backwards feature selection* permitiu reduzir o erro do nosso modelo na percentagem de:

```{r echo = FALSE}
print((mean(r_lda_total) - mean(r_lda_7it))*100)
```
O que representa uma diferença insignificante na precisão do modelo. No entanto, obtemos um modelo muito mais simples que produz resultos muito similares, o que nos indica que este é o conjunto de variáveis explicativas da variação da qualidade. Podemos observar os resultados da variação dos erros da seguinte forma, que nos permitem perceber que, mesmo alterando atributos, os erros continuam iguais.

```{r echo = FALSE}
boxplot(r_lda_total,r_lda_1it,r_lda_2it,r_lda_3it,r_lda_4it,r_lda_5it,r_lda_6it,r_lda_7it, names = c("PP", "1ªI","2ªI","3ªI","4ªI","5ªI","6ªI", "UI"))
```
Em suma, para o conjunto de dados do vinho tinto, o modelo que mostrou ter mais sucesso foi aquele que apenas considerava os seguintes atributos como significativos:

* `volatile.acidity` 
* `total.sulfur.dioxide` 
* `sulphates` 
* `alcohol` 

#### Vinho Branco

De igual forma, partindo de um modelo que considera todos os atributos existentes, aplicamos a mesma metodologia de forma incremental. Inicialmente, com o modelo completo, obtemos o seguinte resultado.

```{r}
w_lda_total = cross_validation_w_lda(white_wine, 10, quality ~ .)
mean(w_lda_total)
```

Com esta metodologia, fomos capazes de obter a seguinte tabela que resume os resultados obtidos.

```{r echo=FALSE}
w_lda_1it = cross_validation_w_lda(white_wine, 10, quality ~ .  - density )
w_lda_2it = cross_validation_w_lda(white_wine, 10, quality ~ . - density - fixed.acidity )
w_lda_3it = cross_validation_w_lda(white_wine, 10, quality ~ . - density - fixed.acidity - citric.acid )
w_lda_4it = cross_validation_w_lda(white_wine, 10, quality ~ . - density - fixed.acidity - citric.acid - pH )
w_lda_5it = cross_validation_w_lda(white_wine, 10, quality ~ . - density - fixed.acidity - citric.acid - pH - total.sulfur.dioxide )
w_lda_6it = cross_validation_w_lda(white_wine, 10, quality ~ . - density - fixed.acidity - citric.acid - pH - total.sulfur.dioxide - free.sulfur.dioxide)
w_lda_7it = cross_validation_w_lda(white_wine, 10, quality ~ . - density - fixed.acidity - citric.acid - pH - total.sulfur.dioxide - free.sulfur.dioxide - chlorides )

base_it <- c("-", mean(w_lda_total))
lda1it <- c("density", mean(w_lda_1it))
lda2it <- c("fixed acidity", mean(w_lda_2it))
lda3it <- c("citric acid", mean(w_lda_3it))
lda4it <- c("pH", mean(w_lda_4it))
lda5it <- c("total sulfur dioxide", mean(w_lda_5it))
lda6it <- c("free sulfur dioxide", mean(w_lda_6it))
lda7it <- c("chlorides", mean(w_lda_7it))
col_names_all = c("Ponto de Partido", "1ª Iteração","2ª Iteração","3ª Iteração","4ª Iteração","5ª Iteração","6ª Iteração", "Última Iteração")
main_df <- data.frame(base_it, lda1it,lda2it,lda3it,lda4it,lda5it,lda6it,lda7it )
names(main_df) <- col_names_all
knitr::kable(t(main_df), col.names=c("Atributo Removido","Precisão Obtida"),caption="Resumo da aplicação de backwards feature selection")
```

Como se pode observar pela tabela acima, a aplicação do método de *backwards feature selection* permitiu reduzir o erro do nosso modelo na percentagem de:

```{r echo = FALSE}
print((mean(w_lda_total) - mean(w_lda_7it))*100)
```
Como sucede no caso do LDA no data set do vinho tinto, a diferença é insignificante, pelo que preferimos o modelo mais simples. Podemos compreender esta variação nos erros pelo seguinte gráfico.

```{r echo = FALSE}
boxplot(w_lda_total,w_lda_1it,w_lda_2it,w_lda_3it,w_lda_4it,w_lda_5it,w_lda_6it,w_lda_7it, names = c("PP", "1ªI","2ªI","3ªI","4ªI","5ªI","6ªI", "UI"))
```
Em suma, para o conjunto de dados do vinho branco, o modelo que mostrou ter mais sucesso foi aquele que apenas considerava os seguintes atributos como significativos:

* `volatile.acidity` 
* `residual.sugar` 
* `sulphates` 
* `alcohol`

### Conclusões

Através do método de LDA, conseguimos chegar a modelos distintos que utilizam diferentes atributos para, dentro do seu data set, prever os resultados de forma ótima. Com este modelo, conseguimos modelar os comportamentos e gostos dos provadores de vinho e conseguimos denotar que:

* Em *vinhos tintos* o nível de álcool é o atributo com mais impacto na previsão da qualidade, sendo o atributo de densidade ignorado. O dióxido de enxofre e sulfatos tem um impacto direto na qualidade sensorial do vinho. Denota-se também que a quantidade de sulfato, que se considerava ser apenas para efeitos anti-oxidantes, tem impacto na qualidade, o que pode indiciar uma alteração do sabor.
* Em *vinhos brancos* o nível de álcool é o atributo com mais impacto na previsão da qualidade, a única diferença de atributos significativos responde ao açúcar residual. Nos vinhos brancos, este atributo é significativo, enquanto que nos vinhos tintos é preferido o dióxido de enxofre total.

Os resultados acima permitem chegar à conclusão que em vinhos brancos o mais importante é a doçura e acidez. Enquanto que em vinhos tintos, os fatores mais importantes são o nível do álcool e quantidade de dióxido de enxofre.





